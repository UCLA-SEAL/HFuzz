[{"path":"/home/u93631/A10_oneapi/apsp/src/apsp_pipe_value_range.cpp", "name":"apsp_pipe_value_range.cpp", "has_active_debug_locs":false, "absName":"/home/u93631/A10_oneapi/apsp/src/apsp_pipe_value_range.cpp", "content":"//==============================================================\u000A// This sample provides a parallel implementation of blocked Floyd Warshall\u000A// algorithm to compute all pairs shortest paths using DPC++.\u000A//==============================================================\u000A// Copyright Â© Intel Corporation\u000A//\u000A// SPDX-License-Identifier: MIT\u000A// =============================================================\u000A\u000A#include <CL/sycl.hpp>\u000A#include <chrono>\u000A#include <cstdlib>\u000A#include <iostream>\u000A#include <math.h>\u000A\u000A// dpc_common.hpp can be found in the dev-utilities include folder.\u000A// e.g., $ONEAPI_ROOT/dev-utilities/<version>/include/dpc_common.hpp\u000A#include \"dpc_common.hpp\"\u000A#if FPGA || FPGA_EMULATOR\u000A  #include <sycl/ext/intel/fpga_extensions.hpp>\u000A#endif\u000A\u000Ausing namespace std;\u000Ausing namespace sycl;\u000A\u000A// Number of nodes in the graph.\u000Aconstexpr int nodes = 1024;\u000A\u000A// Block length and block count (along a single dimension).\u000Aconstexpr int block_length = 16;\u000Aconstexpr int block_count = (nodes / block_length);\u000A\u000A// Maximum distance between two adjacent nodes.\u000Aconstexpr int max_distance = 100;\u000Aconstexpr int infinite = (nodes * max_distance);\u000A\u000A// Number of repetitions.\u000Aconstexpr int repetitions = 8;\u000Aint amax=0;\u000A\u000Adouble value_min=10000;\u000Adouble value_max=0;\u000A\u000A// Randomly initialize directed graph.\u000Avoid InitializeDirectedGraph(int *graph, std::string file) {\u000A  \u000A  std::ifstream read(file);\u000A  if (!read.is_open()){\u000A      std::cout << \"Could not open the input file.\\n\";\u000A  } \u000A    \u000A  for (int i = 0; i < nodes; i++) {\u000A    for (int j = 0; j < nodes; j++) {\u000A      int cell = i * nodes + j;\u000A\u000A      if (i == j) {\u000A        graph[cell] = 0;\u000A      } else if (rand() % 2) {\u000A        graph[cell] = infinite;\u000A      } else {\u000A        int number;\u000A        if (read >> number){\u000A            graph[cell] = number;\u000A        }\u000A        else {\u000A            graph[cell] = rand() % max_distance + 1;\u000A        }\u000A      }\u000A      if (graph[cell]>amax) {amax=graph[cell];}\u000A    }\u000A  }\u000A  \u000A}\u000A\u000A// Copy graph.\u000Avoid CopyGraph(int *to, int *from) {\u000A  for (int i = 0; i < nodes; i++) {\u000A    for (int j = 0; j < nodes; j++) {\u000A      int cell = i * nodes + j;\u000A      to[cell] = from[cell];\u000A    }\u000A  }\u000A}\u000A\u000A// Check if two graphs are equal.\u000Abool VerifyGraphsAreEqual(int *graph, int *h) {\u000A  for (int i = 0; i < nodes; i++) {\u000A    for (int j = 0; j < nodes; j++) {\u000A      int cell = i * nodes + j;\u000A\u000A      if (graph[cell] != h[cell]) {\u000A        return false;\u000A      }\u000A    }\u000A  }\u000A\u000A  return true;\u000A}\u000A\u000A// The basic (sequential) implementation of Floyd Warshall algorithm for\u000A// computing all pairs shortest paths.\u000Avoid FloydWarshall(int *graph) {\u000A  for (int k = 0; k < nodes; k++) {\u000A    for (int i = 0; i < nodes; i++) {\u000A      for (int j = 0; j < nodes; j++) {\u000A        if (graph[i * nodes + j] >\u000A            graph[i * nodes + k] + graph[k * nodes + j]) {\u000A          graph[i * nodes + j] = graph[i * nodes + k] + graph[k * nodes + j];\u000A        }\u000A      }\u000A    }\u000A  }\u000A}\u000A\u000Atypedef accessor<float, 2, access::mode::read_write, access::target::local>\u000A    LocalBlockF;\u000A\u000Atypedef accessor<int, 2, access::mode::read_write, access::target::local>\u000A    LocalBlock;\u000A// Inner loop of the blocked Floyd Warshall algorithm. A thread handles one cell\u000A// of a block. To complete the computation of a block, this function is invoked\u000A// by as many threads as there are cells in the block. Each such invocation\u000A// computes as many iterations as there are blocks (along a single dimension).\u000A// Moreover, each thread (simultaneously operating on a block), synchronizes\u000A// between them at the end of each iteration. This is required for correctness\u000A// as a following iteration depends on the previous iteration.\u000Avoid BlockedFloydWarshallCompute(nd_item<1> &item, const LocalBlock &C,\u000A                                 const LocalBlock &A, const LocalBlock &B,\u000A                                 int i, int j) {\u000A  for (int k = 0; k < block_length; k++) {\u000A    if (C[i][j] > A[i][k] + B[k][j]) {\u000A      C[i][j] = A[i][k] + B[k][j];\u000A    }\u000A\u000A    item.barrier(access::fence_space::local_space);\u000A  }\u000A}\u000A\u000A// Phase 1 of blocked Floyd Warshall algorithm. It always operates on a block\u000A// on the diagonal of the adjacency matrix of the graph.\u000Avoid BlockedFloydWarshallPhase1(queue &q, int *graph, int round) {\u000A  // Each group will process one block.\u000A  constexpr auto blocks = 1;\u000A  // Each item/thread in a group will handle one cell of the block.\u000A  constexpr auto block_size = block_length * block_length;\u000A\u000A  q.submit([&](handler &h) {\u000A    LocalBlock block(range<2>(block_length, block_length), h);\u000A\u000A    h.parallel_for<class KernelPhase1>(\u000A        nd_range<1>(blocks * block_size, block_size), [=](nd_item<1> item) {\u000A          auto tid = item.get_local_id(0);\u000A          auto k = tid / block_length;\u000A          auto j = tid % block_length;\u000A          auto i = k + 0.1;\u000A\u000A          // Copy data to local memory.\u000A          block[k][j] = graph[(round * block_length + k) * nodes +\u000A                              (round * block_length + j)];\u000A          item.barrier(access::fence_space::local_space);\u000A\u000A          // Compute.\u000A          BlockedFloydWarshallCompute(item, block, block, block, i, j);\u000A\u000A          // Copy back data to global memory.\u000A          graph[(round * block_length + k) * nodes +\u000A                (round * block_length + j)] = block[k][j];\u000A          item.barrier(access::fence_space::local_space);\u000A        });\u000A  });\u000A\u000A  q.wait();\u000A}\u000A\u000A// Phase 2 of blocked Floyd Warshall algorithm. It always operates on blocks\u000A// that are either on the same row or on the same column of a diagonal block.\u000Avoid BlockedFloydWarshallPhase2(queue &q, int *graph, int round) {\u000A  // Each group will process one block.\u000A  constexpr auto blocks = block_count;\u000A  // Each item/thread in a group will handle one cell of the block.\u000A  constexpr auto block_size = block_length * block_length;\u000A    \u000A    buffer vmin(&value_min,range<1>(1));\u000A    buffer vmax(&value_max,range<1>(1));\u000A  \u000A  q.submit([&](handler &h) {\u000A    LocalBlock diagonal(range<2>(block_length, block_length), h);\u000A    LocalBlock off_diag(range<2>(block_length, block_length), h);\u000A      \u000A    accessor a(vmin, h, write_only);\u000A    accessor b(vmax, h, write_only);\u000A    h.parallel_for<class KernelPhase2>(\u000A        nd_range<1>(blocks * block_size, block_size), [=](nd_item<1> item) {\u000A          auto gid = item.get_group(0);\u000A          auto index = gid;\u000A\u000A          if (index != round) {\u000A            auto tid = item.get_local_id(0);\u000A            auto i = tid / block_length;\u000A            auto j = tid % block_length;\u000A\u000A            // Copy data to local memory.\u000A            diagonal[i][j] = graph[(round * block_length + i) * nodes +\u000A                                   (round * block_length + j)];\u000A            off_diag[i][j] = graph[(index * block_length + i) * nodes +\u000A                                   (round * block_length + j)];\u000A            item.barrier(access::fence_space::local_space);\u000A\u000A            // Compute for blocks above and below the diagonal block.\u000A            BlockedFloydWarshallCompute(item, off_diag, off_diag, diagonal, i,\u000A                                        j);\u000A\u000A            // Copy back data to global memory.\u000A            graph[(index * block_length + i) * nodes +\u000A                  (round * block_length + j)] = off_diag[i][j];\u000A\u000A            // Copy data to local memory.\u000A            off_diag[i][j] = graph[(round * block_length + i) * nodes +\u000A                                   (index * block_length + j)];\u000A            if (off_diag[i][j]<a[0]){a[0]=off_diag[i][j];}\u000A            if (off_diag[i][j]>b[0]){b[0]=off_diag[i][j];}\u000A            item.barrier(access::fence_space::local_space);\u000A\u000A            // Compute for blocks at left and at right of the diagonal block.\u000A            BlockedFloydWarshallCompute(item, off_diag, diagonal, off_diag, i,\u000A                                        j);\u000A\u000A            // Copy back data to global memory.\u000A            graph[(round * block_length + i) * nodes +\u000A                  (index * block_length + j)] = off_diag[i][j];\u000A            item.barrier(access::fence_space::local_space);\u000A          }\u000A        });\u000A  });\u000A\u000A  q.wait();\u000A}\u000A\u000A// Phase 3 of blocked Floyd Warshall algorithm. It operates on all blocks except\u000A// the ones that are handled in phase 1 and in phase 2 of the algorithm.\u000Avoid BlockedFloydWarshallPhase3(queue &q, int *graph, int round) {\u000A  // Each group will process one block.\u000A  constexpr auto blocks = block_count * block_count;\u000A  // Each item/thread in a group will handle one cell of the block.\u000A  constexpr auto block_size = block_length * block_length;\u000A\u000A  q.submit([&](handler &h) {\u000A    LocalBlock A(range<2>(block_length, block_length), h);\u000A    LocalBlock B(range<2>(block_length, block_length), h);\u000A    LocalBlock C(range<2>(block_length, block_length), h);\u000A\u000A    h.parallel_for<class KernelPhase3>(\u000A        nd_range<1>(blocks * block_size, block_size), [=](nd_item<1> item) {\u000A          auto bk = round;\u000A\u000A          auto gid = item.get_group(0);\u000A          auto bi = gid / block_count;\u000A          auto bj = gid % block_count;\u000A\u000A          if ((bi != bk) && (bj != bk)) {\u000A            auto tid = item.get_local_id(0);\u000A            auto i = tid / block_length;\u000A            auto j = tid % block_length;\u000A\u000A            // Copy data to local memory.\u000A            A[i][j] = graph[(bi * block_length + i) * nodes +\u000A                            (bk * block_length + j)];\u000A            B[i][j] = graph[(bk * block_length + i) * nodes +\u000A                            (bj * block_length + j)];\u000A            C[i][j] = graph[(bi * block_length + i) * nodes +\u000A                            (bj * block_length + j)];\u000A\u000A            item.barrier(access::fence_space::local_space);\u000A\u000A            // Compute.\u000A            BlockedFloydWarshallCompute(item, C, A, B, i, j);\u000A\u000A            // Copy back data to global memory.\u000A            graph[(bi * block_length + i) * nodes + (bj * block_length + j)] =\u000A                C[i][j];\u000A            item.barrier(access::fence_space::local_space);\u000A          }\u000A        });\u000A  });\u000A\u000A  q.wait();\u000A}\u000A\u000A// Parallel implementation of blocked Floyd Warshall algorithm. It has three\u000A// phases. Given a prior round of these computation phases are complete, phase 1\u000A// is independent; Phase 2 can only execute after phase 1 completes; Similarly\u000A// phase 3 depends on phase 2 so can only execute after phase 2 is complete.\u000A//\u000A// The inner loop of the sequential implementation is similar to:\u000A//   g[i][j] = min(g[i][j], g[i][k] + g[k][j])\u000A// A careful observation shows that for the kth iteration of the outer loop,\u000A// the computation depends on cells either on the kth column, g[i][k] or on the\u000A// kth row, g[k][j] of the graph. Phase 1 handles g[k][k], phase 2 handles\u000A// g[*][k] and g[k][*], and phase 3 handles g[*][*] in that sequence. This cell\u000A// level observations largely propagate to the blocks as well.\u000Avoid BlockedFloydWarshall(queue &q, int *graph) {\u000A  for (int round = 0; round < block_count; round++) {\u000A    BlockedFloydWarshallPhase1(q, graph, round);\u000A    BlockedFloydWarshallPhase2(q, graph, round);\u000A    BlockedFloydWarshallPhase3(q, graph, round);\u000A  }\u000A}\u000A\u000Aint main(int argc, char* argv[]) {\u000A    // Create device selector for the device of your interest.\u000A#if FPGA_EMULATOR\u000A  // DPC++ extension: FPGA emulator selector on systems without FPGA card.\u000A  ext::intel::fpga_emulator_selector d_selector;\u000A#elif FPGA\u000A  // DPC++ extension: FPGA selector on systems with FPGA card.\u000A  ext::intel::fpga_selector d_selector;\u000A#else\u000A  // The default device selector will select the most performant device.\u000A  default_selector d_selector;\u000A#endif\u000A  std::string file;\u000A  if (argc > 1) file = argv[1];\u000A\u000A  try {\u000A    queue q{d_selector, dpc_common::exception_handler};\u000A    auto device = q.get_device();\u000A    auto work_group_size = device.get_info<info::device::max_work_group_size>();\u000A    auto block_size = block_length * block_length;\u000A\u000A    cout << \"Device: \" << device.get_info<info::device::name>() << \"\\n\";\u000A\u000A    if (work_group_size < block_size) {\u000A      cout << \"Work group size \" << work_group_size\u000A           << \" is less than required size \" << block_size << \"\\n\";\u000A      return -1;\u000A    }\u000A\u000A    // Allocate unified shared memory so that graph data is accessible to both\u000A    // the CPU and the device (e.g., a GPU).\u000A    int *graph = (int *)malloc(sizeof(int) * nodes * nodes);\u000A    int *sequential = malloc_shared<int>(nodes * nodes, q);\u000A    int *parallel = malloc_shared<int>(nodes * nodes, q);\u000A\u000A    if ((graph == nullptr) || (sequential == nullptr) ||\u000A        (parallel == nullptr)) {\u000A      if (graph != nullptr) free(graph);\u000A      if (sequential != nullptr) free(sequential, q);\u000A      if (parallel != nullptr) free(parallel, q);\u000A\u000A      cout << \"Memory allocation failure.\\n\";\u000A      return -1;\u000A    }\u000A\u000A    // Initialize directed graph.\u000Adouble t1;\u000Adpc_common::TimeInterval timer_p;\u000At1 = timer_p.Elapsed();\u000A    InitializeDirectedGraph(graph, file);\u000Adouble exec_time=timer_p.Elapsed()-t1;\u000A    cout<<\"IO time: \"<<exec_time<<\"\\n\";\u000A\u000A    // Warm up the JIT.\u000A    CopyGraph(parallel, graph);\u000A    BlockedFloydWarshall(q, parallel);\u000A\u000A    // Measure execution times.\u000A    double elapsed_s = 0;\u000A    double elapsed_p = 0;\u000A    int i;\u000A\u000A    cout << \"Repeating computation \" << repetitions\u000A         << \" times to measure run time ...\\n\";\u000A\u000A    for (i = 0; i < repetitions; i++) {\u000A      cout << \"Iteration: \" << (i + 1) << \"\\n\";\u000A\u000A      // Sequential all pairs shortest paths.\u000A      CopyGraph(sequential, graph);\u000A\u000A      dpc_common::TimeInterval timer_s;\u000A\u000A      FloydWarshall(sequential);\u000A      elapsed_s += timer_s.Elapsed();\u000A\u000A      // Parallel all pairs shortest paths.\u000A      CopyGraph(parallel, graph);\u000A\u000A      dpc_common::TimeInterval timer_p;\u000A\u000A      BlockedFloydWarshall(q, parallel);\u000A      elapsed_p += timer_p.Elapsed();\u000A\u000A      // Verify two results are equal.\u000A      if (!VerifyGraphsAreEqual(sequential, parallel)) {\u000A        cout << \"Failed to correctly compute all pairs shortest paths!\\n\";\u000A        break;\u000A      }\u000A    }\u000A\u000A    if (i == repetitions) {\u000A      cout << \"Successfully computed all pairs shortest paths in parallel!\\n\";\u000A\u000A      elapsed_s /= repetitions;\u000A      elapsed_p /= repetitions;\u000A      std::ofstream outfile;\u000A      outfile.open(\"exec_fpga_info.txt\");\u000A      outfile << value_max << std::endl << value_max << std::endl << value_max;\u000A      outfile.close();\u000A\u000A      cout << \"Time sequential: \" << elapsed_s << \" sec\\n\";\u000A      cout << \"Time parallel: \" << elapsed_p << \" sec\\n\";\u000A    }\u000A\u000A    // Free unified shared memory.\u000A    free(graph);\u000A    free(sequential, q);\u000A    free(parallel, q);\u000A    exec_time=timer_p.Elapsed()-t1;\u000A    cout<<\"final time: \"<<exec_time<<\"\\n\";\u000A    cout<<\"Max:\"<<value_max<<\"\\n\";\u000A    cout<<\"Min:\"<<value_min<<\"\\n\";\u000A    \u000A  } catch (std::exception const &e) {\u000A    cout << \"An exception is caught while computing on device.\\n\";\u000A    terminate();\u000A  }\u000A\u000A  return 0;\u000A}\u000A"}, {"path":"/glob/development-tools/versions/oneapi/2022.3/oneapi/dev-utilities/2021.7.0/include/dpc_common.hpp", "name":"dpc_common.hpp", "has_active_debug_locs":false, "absName":"/glob/development-tools/versions/oneapi/2022.3/oneapi/dev-utilities/2021.7.0/include/dpc_common.hpp", "content":"// Copyright (C) 2020 Intel Corporation\u000A// SPDX-License-Identifier: MIT\u000A\u000A#ifndef _DP_HPP\u000A#define _DP_HPP\u000A\u000A#include <stdlib.h>\u000A#include <exception>\u000A\u000A#include <CL/sycl.hpp>\u000A\u000Anamespace dpc_common {\u000A// This exception handler will catch async exceptions\u000Astatic auto exception_handler = [](cl::sycl::exception_list eList) {\u000A  for (std::exception_ptr const &e : eList) {\u000A    try {\u000A      std::rethrow_exception(e);\u000A    } catch (std::exception const &e) {\u000A#if _DEBUG\u000A      std::cout << \"Failure\" << std::endl;\u000A#endif\u000A      std::terminate();\u000A    }\u000A  }\u000A};\u000A\u000A// The TimeInterval is a simple RAII class.\u000A// Construct the timer at the point you want to start timing.\u000A// Use the Elapsed() method to return time since construction.\u000A\u000Aclass TimeInterval {\u000A public:\u000A  TimeInterval() : start_(std::chrono::steady_clock::now()) {}\u000A\u000A  double Elapsed() {\u000A    auto now = std::chrono::steady_clock::now();\u000A    return std::chrono::duration_cast<Duration>(now - start_).count();\u000A  }\u000A\u000A private:\u000A  using Duration = std::chrono::duration<double>;\u000A  std::chrono::steady_clock::time_point start_;\u000A};\u000A\u000A};  // namespace dpc_common\u000A\u000A#endif\u000A"}]